{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file uses transfer learning via the VGG16 convnet to build a model to identify \n",
    "# the urgency of each images as coded by Brett\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from scipy.misc import imresize\n",
    "\n",
    "with open('feat_vecs.pkl', 'rb') as f:\n",
    "\tfeature_vecs_old= pickle.load(f)\n",
    "with open('Hurricane_Harvey/y_urgency.pkl', 'rb') as f:\n",
    "\tY_urgency_old = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246\n",
      "2246\n",
      "2242\n",
      "2242\n",
      "set(['1', '0', '3', '2', '4'])\n"
     ]
    }
   ],
   "source": [
    "print len(feature_vecs_old)\n",
    "print len(Y_urgency_old)\n",
    "\n",
    "feature_vecs = []\n",
    "Y_urgency = []\n",
    "for a, b in zip(feature_vecs_old, Y_urgency_old):\n",
    "    if b != u' ':\n",
    "        feature_vecs.append(a)\n",
    "        Y_urgency.append(b)\n",
    "        \n",
    "print len(feature_vecs)\n",
    "print len(Y_urgency)\n",
    "\n",
    "print set(Y_urgency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading VGG16 model weights\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vecs = np.asarray(feature_vecs)\n",
    "# flattening the layers to conform to MLP input\n",
    "X=feature_vecs.reshape(2242, 25088)\n",
    "# converting target variable to array\n",
    "Y=np.asarray(Y_urgency)\n",
    "#performing one-hot encoding for the target variable\n",
    "Y=pd.get_dummies(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1793, 25088)\n",
      "(1793, 5)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#creating training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "\n",
    "print X_train.shape\n",
    "print Y_train.shape\n",
    "print type(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  96/1793 [>.............................] - ETA: 1:23:22 - loss: 1.5912 - acc: 0.2917"
     ]
    }
   ],
   "source": [
    "# creating a mlp model\n",
    "from keras.layers import Dense, Activation\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(10000, input_dim=25088, activation='relu',kernel_initializer='uniform'))\n",
    "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(5000,input_dim=10000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(1000,input_dim=5000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(750,input_dim=1000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(300,input_dim=750,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)\n",
    "\n",
    "# number of output classification categories\n",
    "model.add(Dense(units=5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# fitting the model \n",
    "history = model.fit(np.asarray(X_train), np.asarray(Y_train), epochs=5,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3481914, 0.518931]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = np.asarray(X_valid)\n",
    "Y_valid = np.asarray(Y_valid)\n",
    "\n",
    "model.test_on_batch(X_valid, Y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
